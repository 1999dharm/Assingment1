{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3bde00-8418-4de8-bb9c-011ee22ae44c",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "\n",
    "Ans In machine learning algorithms, polynomial functions and kernel functions are related in that kernel functions can be used to implicitly represent high-dimensional polynomial functions.\n",
    "\n",
    "A polynomial function is a mathematical function that is defined as a sum of powers in one or more variables. In the context of machine learning, polynomial functions are often used to represent complex decision boundaries in high-dimensional feature spaces. For example, a quadratic polynomial function can be used to represent a parabolic decision boundary in a 2D feature space, while a cubic polynomial function can be used to represent a more complex decision boundary.\n",
    "\n",
    "However, computing polynomial functions in high-dimensional feature spaces can be computationally expensive and require a lot of memory. This is where kernel functions come in. Kernel functions are a type of similarity function that can be used to compute dot products in high-dimensional feature spaces without actually computing the coordinates of the data points in that space. In other words, kernel functions provide a way to implicitly represent high-dimensional feature spaces without explicitly computing the coordinates of the data points in those spaces.\n",
    "\n",
    "One common type of kernel function is the polynomial kernel function, which is defined as:\n",
    "\n",
    "K(x, y) = (x * y + c)^d\n",
    "\n",
    "where x and y are the data points, c is a constant, and d is the degree of the polynomial. The polynomial kernel function can be used to represent a polynomial function of degree d in the feature space.\n",
    "\n",
    "In summary, kernel functions provide a way to implicitly represent high-dimensional feature spaces without explicitly computing the coordinates of the data points in those spaces. Polynomial functions can be represented using kernel functions, such as the polynomial kernel function, which can be used to represent polynomial functions of different degrees in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cdb1d-aab1-4ecc-925e-1344d8333947",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit- learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c3103d-08aa-42d2-a917-b749a06f0349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM with a polynomial kernel\n",
    "poly_svm = SVC(kernel='poly', degree=3, coef0=1, C=1)\n",
    "\n",
    "# Fit the SVM to the training data\n",
    "poly_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "y_pred = poly_svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the SVM\n",
    "accuracy = poly_svm.score(X_test, y_test)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486b9dd-2edb-49f3-866f-9b38cef06f9c",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "Ans In Support Vector Regression (SVR), epsilon is a hyperparameter that controls the width of the epsilon-insensitive zone around the regression line. The epsilon-insensitive zone is a buffer zone around the regression line within which errors are not penalized.\n",
    "\n",
    "As we increase the value of epsilon, the width of the epsilon-insensitive zone increases. This means that more training instances fall within this zone and are not used as support vectors. Therefore, increasing the value of epsilon tends to decrease the number of support vectors in SVR.\n",
    "\n",
    "However, it is important to note that the relationship between epsilon and the number of support vectors in SVR is not linear and may also depend on the complexity of the data and the choice of kernel function. In some cases, increasing epsilon may result in more support vectors, while in other cases, decreasing epsilon may result in fewer support vectors.\n",
    "\n",
    "In general, the choice of the value of epsilon in SVR involves a trade-off between the complexity of the model and its generalization performance. A larger value of epsilon results in a simpler model with fewer support vectors, but may result in higher bias and lower accuracy. On the other hand, a smaller value of epsilon results in a more complex model with more support vectors, but may result in lower bias and higher accuracy. The optimal value of epsilon depends on the specific problem and needs to be determined through experimentation and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec18a3a-3aea-4114-b2db-baae4bc3e9c7",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "Ans The performance of Support Vector Regression (SVR) is affected by several parameters, including the choice of kernel function, C parameter, epsilon parameter, and gamma parameter.\n",
    "\n",
    "Kernel function: The choice of kernel function determines how the input data is mapped into a high-dimensional feature space. Different kernel functions have different properties and are suitable for different types of data. Some commonly used kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "Linear kernel: This kernel function is used for linearly separable data. It is the simplest kernel and has the least number of hyperparameters.\n",
    "\n",
    "Polynomial kernel: This kernel function is used for data that is not linearly separable. It has two hyperparameters: degree and gamma. Increasing the degree parameter makes the polynomial function more complex, while increasing the gamma parameter makes the kernel function more sensitive to variations in the data.\n",
    "\n",
    "RBF kernel: This kernel function is used for data that is not linearly separable and has a non-linear relationship between the features. It has one hyperparameter: gamma. Increasing the gamma parameter makes the kernel function more sensitive to variations in the data.\n",
    "\n",
    "Sigmoid kernel: This kernel function is used for data that has a non-linear relationship between the features. It has two hyperparameters: gamma and coef0. Increasing the gamma parameter makes the kernel function more sensitive to variations in the data, while increasing the coef0 parameter shifts the sigmoid function.\n",
    "\n",
    "C parameter: The C parameter controls the trade-off between achieving a low training error and a low testing error. A large value of C will result in a smaller margin, which means that the model will be more complex and will have more support vectors. A smaller value of C will result in a larger margin, which means that the model will be simpler and will have fewer support vectors.\n",
    "\n",
    "Epsilon parameter: The epsilon parameter controls the width of the epsilon-insensitive zone around the regression line. A larger value of epsilon will result in a wider epsilon-insensitive zone, which means that more training instances will be considered to have zero error and will not be used as support vectors.\n",
    "\n",
    "Gamma parameter: The gamma parameter controls the width of the RBF kernel. A larger value of gamma will result in a narrower kernel, which means that the model will be more sensitive to variations in the data.\n",
    "\n",
    "To determine the optimal values of these parameters, one can use techniques such as grid search or randomized search to explore the hyperparameter space and find the combination of values that results in the best performance on a validation set.\n",
    "\n",
    "As an example, suppose we are trying to predict the price of a house based on its features such as square footage, number of bedrooms, and location. If the relationship between the features is highly non-linear, we may want to use an RBF kernel instead of a linear kernel. We can also adjust the value of gamma to control the sensitivity of the kernel to variations in the data. If we have a large training dataset and want a more complex model, we can increase the C parameter to reduce the margin. Conversely, if we have a smaller training dataset and want a simpler model with fewer support vectors, we can decrease the C parameter. Finally, we can adjust the value of epsilon to control the width of the epsilon-insensitive zone based on the desired level of error tolerance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1950d929-d0df-417c-bf00-e290040f50ad",
   "metadata": {},
   "source": [
    "5 Assignemnt\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92d2febf-6f0f-4053-b188-28a10e8a8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7095d7d-a73b-4ffe-ac30-e4f87b0d9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe5c57d-48f2-4f1d-b16a-35234348d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "148c2bf6-38f7-44fa-9f0a-022485ba8087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2d7404f-1a36-4798-9a1f-15177548f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a61b63a5-23b8-4be8-bb7d-2b8e95dfd9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d22f6d64-e73b-4f9a-b7d4-335b317fb56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'degree': 2, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3]}\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1347a7c0-b39a-4ef0-90b8-9e1c21681a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, degree=2, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, degree=2, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, degree=2, kernel='linear')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_tuned = grid_search.best_estimator_\n",
    "svc_tuned.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "600fb787-c0e3-4646-93e1-d0da470c337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svc_tuned, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d38d47-cf93-4d43-a30f-bbfcd73ab802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
